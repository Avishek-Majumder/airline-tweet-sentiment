{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Airline Tweet Sentiment – Exploratory Data Analysis\n",
        "\n",
        "This notebook provides a quick exploratory data analysis (EDA) for the airline tweet sentiment datasets used in our paper:\n",
        "\n",
        "**Exploring Transformer Models for Sentiment Analysis in Airline Service Reviews**  \n",
        "https://ieeexplore.ieee.org/abstract/document/10796289\n",
        "\n",
        "We focus on:\n",
        "- Basic dataset structure and size\n",
        "- Sentiment label distribution\n",
        "- Example tweets per sentiment class\n",
        "\n",
        "You can switch between datasets (e.g. `airline_us`, `airline_global`, `airline_merged`) by changing a single variable in the first code cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "setup"
        ]
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from airline_sentiment.utils.config import load_global_config, PROJECT_ROOT\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Configuration\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "# Choose which dataset to inspect: \"airline_us\", \"airline_global\", \"airline_merged\"\n",
        "DATASET_NAME = \"airline_us\"\n",
        "\n",
        "cfg = load_global_config()\n",
        "processed_root = cfg.get(\"paths\", {}).get(\"processed_data\", \"data/processed\")\n",
        "dataset_dir = PROJECT_ROOT / processed_root / DATASET_NAME\n",
        "\n",
        "print(f\"Using processed dataset from: {dataset_dir}\")\n",
        "\n",
        "if not dataset_dir.is_dir():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Processed dataset directory not found. Run 'python scripts/prepare_datasets.py' first.\"\n",
        "    )\n",
        "\n",
        "train_path = dataset_dir / \"train.csv\"\n",
        "val_path = dataset_dir / \"val.csv\"\n",
        "test_path = dataset_dir / \"test.csv\"\n",
        "\n",
        "for p in [train_path, val_path, test_path]:\n",
        "    if not p.is_file():\n",
        "        raise FileNotFoundError(\n",
        "            f\"Expected split file not found: {p}. Did you run the data preparation script?\"\n",
        "        )\n",
        "\n",
        "train_df = pd.read_csv(train_path)\n",
        "val_df = pd.read_csv(val_path)\n",
        "test_df = pd.read_csv(test_path)\n",
        "\n",
        "len(train_df), len(val_df), len(test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic dataset overview\n",
        "\n",
        "We start by looking at the shapes of the splits and a sample of rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "overview"
        ]
      },
      "outputs": [],
      "source": [
        "print(\"Train shape:\", train_df.shape)\n",
        "print(\"Val shape:  \", val_df.shape)\n",
        "print(\"Test shape: \", test_df.shape)\n",
        "\n",
        "print(\"\\nTrain columns:\", list(train_df.columns))\n",
        "\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sentiment label distribution\n",
        "\n",
        "Now we inspect the distribution of sentiment labels in the training set. This helps us understand class imbalance and verify that our stratified splitting worked as expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "label-distribution"
        ]
      },
      "outputs": [],
      "source": [
        "label_counts = train_df[\"label_str\"].value_counts().sort_index()\n",
        "label_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "label-distribution-plot"
        ]
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6, 4))\n",
        "label_counts.plot(kind=\"bar\")\n",
        "plt.title(f\"Label distribution in TRAIN split – {DATASET_NAME}\")\n",
        "plt.xlabel(\"Sentiment label\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example tweets per sentiment class\n",
        "\n",
        "We inspect a few example tweets from each sentiment class to qualitatively understand the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "examples"
        ]
      },
      "outputs": [],
      "source": [
        "def show_examples(df, label_str, n=5):\n",
        "    subset = df[df[\"label_str\"] == label_str].head(n)\n",
        "    print(f\"\\n=== Examples for label: {label_str} (n={len(subset)}) ===\")\n",
        "    for i, row in subset.iterrows():\n",
        "        print(f\"[{i}]\", row[\"text\"])\n",
        "\n",
        "for label in sorted(train_df[\"label_str\"].unique()):\n",
        "    show_examples(train_df, label, n=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next steps\n",
        "\n",
        "From here you can extend the EDA to:\n",
        "- Analyze tweet length distributions\n",
        "- Investigate frequent unigrams/bigrams per class\n",
        "- Explore temporal patterns (if timestamps are available)\n",
        "- Check for data leakage or anomalies\n",
        "\n",
        "This notebook is meant as a lightweight, reproducible starting point for exploring the airline tweet sentiment datasets used in our experiments."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
